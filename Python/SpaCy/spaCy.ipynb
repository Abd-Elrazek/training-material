{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a language model, English in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Part of speech tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read a text file into a string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('Data/frost.txt') as file:\n",
    "    text = ''.join(file.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Parse the text using the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = en_nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Show the part of speech tags, as well as the context of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "    print(f'{word.text!r}: {word.pos_}, '\n",
    "          f'{word.left_edge.text!r} <- {word.head.text!r} -> {word.right_edge.text!r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we can't use backslashes in f-strings, we define a constant to represent it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "newline = '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To split a text in sentences, a statistical model is used that was obtained from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(doc.sents):\n",
    "    print(f'{i:3d} {sentence.text.replace(newline, \" \")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For poetry, sentences seem somewhat hard to detect.  However, it is possible to define a language model for English and add a rule-based sentencizer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_nlp_alt = spacy.lang.en.English()\n",
    "sentencizer = en_nlp_alt.create_pipe('sentencizer')\n",
    "en_nlp_alt.add_pipe(sentencizer)\n",
    "doc = en_nlp_alt(text)\n",
    "for i, sentence in enumerate(doc.sents):\n",
    "    print(f'{i:3d} {sentence.text.replace(newline, \" \").strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By way of example, consider Plato's *Republic*.  This is a fairly long text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l -w Data/republic.mb.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/republic.mb.txt') as file:\n",
    "    text = ''.join(file.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full result of the language model parsing this text would be rather large, but for our purposes, we require only tokenization, not POS or NER, hence we disable these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = en_nlp(text, disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform lemmatization on all words that are not stop words, and we also eliminate named entities (`-PROP-` as value for lemma) and punctuation.  On the resulting list, a word count is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = en_nlp.Defaults.stop_words | {'\\n', '\\n\\n', '-PRON-'}\n",
    "punctuation = ',.;?!:-'\n",
    "counts = collections.Counter([token.lemma_.lower() for token in doc\n",
    "                              if token.lemma_ not in stopwords and token.lemma_ not in punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-20 words are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distr(counts, nr_words):\n",
    "    words = list()\n",
    "    numbers = list()\n",
    "    for word, number in counts.most_common(nr_words):\n",
    "        words.append(word)\n",
    "        numbers.append(number)\n",
    "    figure, axes = plt.subplots(1, 1, figsize=(15, 5))\n",
    "    axes.bar(words, numbers)\n",
    "    axes.set_xticklabels(words, rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distr(counts, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entiry recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition is supported as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Music by Johann Sebastian Bach is better than that by Friederich Buxtehude. Both lived in Germany'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = en_nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(doc):\n",
    "    print(f'{i:3d} {word.text!r}: {word.pos_}, {word.ent_type_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to retrieve named entities from the document explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print(f'{entity} ({entity.label_}): {entity.start} -> {entity.end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be visualized as markup in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='dep', jupyter=True,\n",
    "                      options={'distance': 140, 'compact': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document similarity can also be computed conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = en_nlp('The book is nice')\n",
    "doc2 = en_nlp('The novel is beautiful')\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = en_nlp('The book is nice')\n",
    "doc2 = en_nlp('The house is on fire')\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['queen', 'lady', 'girl', 'king', 'lord', 'boy', 'cat', 'dog', 'lion']\n",
    "similarity = np.empty((len(words), len(words)))\n",
    "for i, word1 in enumerate(words):\n",
    "    for j, word2 in enumerate(words):\n",
    "        similarity[i, j] =  en_nlp(word1).similarity(en_nlp(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix can be visualized as a heat map using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_matrix(sim, words, cmap=plt.cm.Blues):\n",
    "    figure, axes = plt.subplots(figsize=(6, 6))\n",
    "    axes.imshow(sim, interpolation='nearest', cmap=cmap)\n",
    "    axes.set_xticks(range(len(words)))\n",
    "    axes.set_xticklabels(words, rotation=45)\n",
    "    axes.set_yticks(range(len(words)))\n",
    "    axes.set_yticklabels(words)\n",
    "    fmt = '{0:.2f}'\n",
    "    thresh = 0.5*(sim.max() + sim.min())\n",
    "    for i, j in itertools.product(range(sim.shape[0]), range(sim.shape[1])):\n",
    "        axes.text(j, i, fmt.format(sim[i, j]),\n",
    "                  horizontalalignment=\"center\",\n",
    "                  color=\"white\" if sim[i, j] > thresh else \"black\",\n",
    "                  fontsize=8)\n",
    "    figure.tight_layout()\n",
    "    axes.set_xlabel('word 1')\n",
    "    axes.set_ylabel('word 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity_matrix(similarity, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
